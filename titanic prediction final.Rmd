---
title: 'Kaggle Titanic Survivor Prediction: Comparison of Machine Learning Methods'
author: "Jim Nelson"
date: "Friday, December 11, 2015"
output: html_document
---

### OBJECTIVES
**1. Demonstrate proficiency in R.  
2. Demonstrate ability to perform appropriate data transformations and creative feature engineering.  
3. Compare performace of several R Machine Learning packages.  
4. Submit models to [Kaggle](www.kaggle.com/c/titanic) to assess performance and obtain challenge ranking.**    


### INTRODUCTION
The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.    
One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.  

The goal of this study was to utilize the Kaggle Titanic Challenge dataset to perform and compare several machine learning predict analytic methods to predict which passengers survived the tragedy.  


### METHODS
### Kaggle Titanic Competition
The crowdsourcing predicitve modeling competition website Kaggle was used as a platform for this study, providing a means to assess my skills compared to other participants. As of 12/11/15 there were 3946 participants with 2458 scripts. A good description of how Kaggle works is found [here](en.wikipedia.org/wiki/Kaggle). The [Titanic competition](www.kaggle.com/c/titanic)is an active "Getting Started"competition in the "Knowledge" category which has been ongoing since September 2012.  Kaggle also makes available a forum for competition discussionand a repository for scripts and notebooks used in the competition. All Kaggle rules and instructions were followed during this study.   

### Datasets
Datasets used in this study were obtained from the [Kaggle website](www.kaggle.com/c/titanic/data) as CSV files. The "training" dataset consisted of 891 passengers with the following 14 variables: *PassengerId,Survived,Pclass,Name,Sex,Age*,  
 *SibSp,Parch,Ticket,Fare,Cabin,Embarked*. The "test"dataset contained 418 passengers and the same variables with the exception of the*Survived* variable. A complete description of the variables is shown [here](www.kaggle.com/c/titanic/data). The two datasets were combined to facilitate data curation then seperated for predicitve modeling use.

### Software and Computing Environment
The study was performed locally on a HP Pavilion 23 All-in-One with a 64 Bit OS running Windows 10 with 4.0 GB Ram and an AMD AP-5300 APU processor with Radeon HD graphics. The study was performed in R (R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk" Platform: x86_64-w64-mingw32/x64 (64-bit))Copyright (C) 2015 The R Foundation for Statistical Computing) using the open source R platform R Studio (Version 0.98.1102 - © 2009-2014 RStudio, Inc.)  

The following R software packages with corresponding manuals and vignettes were obtained from the The Comprehensive R Archive Network (CRAN):  
 
__Data Manipulation and Visualization:__    
   *dplyr*: A Grammar of Data Manipulation (v.0.4.3);  
   *ggplot2*: An Implementation of the Grammar of Graphics (v.1.01)    

__Logistic Regression Analysis:__    
   *glm2*: Fitting Generalized Linear Models (v. 1.1.2)    

__Classification statistics and AUROC analysis:__    
   *caret*: Classification and Regression Training (v.6.0-62);  *pROC*: Display and Analyze ROC Curves (v. 1.8)    

__Recursive Partitioning Modeling:__  
   *rpart*: Recursive Partitioning and Regression Trees (v.4.1-10);  
   *rattle*: Graphical User Interface for Data Mining in R (v.4.0.5);  
   *rpart.plot*: Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'(v.1.5.3);  
   *RColorBrewer*: ColorBrewer Palettes (v 1.1-2)  

__Random Forest Modeling:__  
   *randomForest*: Breiman and Cutler's Random Forests for Classification and Regression)    

### DATA CURATION  
### Load Datasets and R Packages

#### Load dplyr package for data transformation

```{r}
library(dplyr)
```

#### Load ggplot2 for data visualization

```{r}
library (ggplot2)
```


#### Load the training and test datasets

```{r}
train <- read.csv ("train.csv",  header= TRUE)
  str(train)
test<- read.csv ("test.csv",  header= TRUE)
  str(test)
```


### Data Transformation and Verification

#### Change the *Survived* variable from int to factor before combining

```{r}
train$Survived<- factor(train$Survived)
```

#### Create *Survived* dummy variable in test set before combining

```{r}
test<- mutate(test, Survived = "none")
```

#### Create sorting variable *dataset* before combining
```{r}
test <- mutate(test, dataset = "testset")
train <- mutate(train, dataset = "trainset")
```


#### Combine training and test datasets for feature engineering

```{r}
titanic.combined <- rbind(test, train)
str(titanic.combined)
```
#### Rename and create local data frame for simplicity

```{r}
data<- tbl_df (titanic.combined)
```

#### Factorize *Pclass, dataset and Survived* variables

```{r}
data$Pclass <- factor(data$Pclass)
data$dataset <- factor(data$dataset)
data$Survived<- factor(data$Survived)
```

#### Check for duplicates
```{r}
IDdups <- distinct(data, PassengerId)
dim(IDdups)
Namedups <- distinct(data, Name)
dim(Namedups)
```
Since there are only 1307 distinct names in the dataset, there may be 2 duplicates. However there are 1309 distinct Passenger ID's


```{r eval=FALSE}
filter(data, duplicated(Name)) 
```

```{r}
filter(data, grepl('Kelly|Connolly', Name, Age ))
```
The different age and ticket numbers of the potential duplicates indicate these are different with the same name not duplicates. 


### Data Exploration

#### Descriptive stats for the data

```{r}
summary(tbl_df(data))
head(data)
```

__1.  overall *Age* and *Cabin* variables are missing ~20% of values__  
__2. *Fare* is missing 1 value__  
__3. *Embarked* is missing 2 values__    

#### Visualize some potentially important features as a function of survival 

##### Age

```{r results ='hide'}
trainset<-data%>% arrange(dataset)%>%slice(419:1309)
head (trainset)
glimpse(trainset)
```

```{r }
hist_Age <- ggplot(trainset, aes(x=Age, fill=Survived))
  hist_Age + geom_bar() # defaults to stacking
  hist_Age + geom_bar(position= "fill") #proportions
```



##### Sex

```{r}
hist_Sex <- ggplot(trainset, aes(x=Sex, fill=Survived))
  hist_Sex + geom_bar(position= "fill") # defaults to stacking
  hist_Sex + geom_bar(position= "fill") #proportions
```

##### Pclass (cabin type)

```{r }
hist_Pclass <- ggplot(trainset, aes(x=Pclass, fill=Survived))
  hist_Pclass + geom_bar() # defaults to stacking 
  hist_Pclass + geom_bar(position= "fill") #proportions
```

##### SibSp (no. siblings/spouse)

```{r}
hist_SibSp <- ggplot(trainset, aes(x=SibSp, fill=Survived, binwidth = .0005))
  hist_SibSp + geom_bar() # defaults to stacking
  hist_SibSp + geom_bar(position= "fill") #proportions  
```

##### Parch (no. parents/children)

```{r}
hist_Parch <- ggplot(trainset, aes(x=Parch, fill=Survived))
  hist_Parch + geom_bar() # defaults to stacking
  hist_Parch + geom_bar(position= "fill") #proportions  
```

### Feature Engineering

#### *Hypothesis 1:* data visualization suggests being a child and/or a female increased your odds of survival

#### Create feature *Child* from feature *Age* <= 16 yrs

```{r}
data <- data %>%
  mutate(Child = Age <=16) 
data$Child <- factor(data$Child)
  glimpse (data)
```


#### Visualize survival as a function of Child
```{r}
trainset<-data%>% arrange(dataset)%>%slice(419:1309)

hist_Child <- ggplot(trainset, aes(x=Child, fill=Survived))
  hist_Child + geom_bar() # defaults to stacking
  hist_Child + geom_bar(position= "fill") #proportions  
```

#### *Hypothesis 2:* Did a persons Title effect survivability?

#### Create new feature called *Title* based on the *Name* feature 


```{r results='hide'}

Mr<-filter(data, grepl('Mr.' ,Name, fixed=TRUE ))
  Mr<-mutate(Mr, title = 'Mr')

Mrs<-filter(data, grepl('Mrs.', Name, fixed=TRUE ))
  Mrs<-mutate(Mrs, title = 'Mrs')

Miss<-filter(data, grepl('Miss.', Name, fixed=TRUE ))
  Miss<-mutate(Miss, title = 'Miss')

Master<-filter(data, grepl('Master.', Name, fixed=TRUE  ))
  Master<-mutate(Master, title = 'Master')

Dr <-filter(data, grepl('Dr.', Name, fixed=TRUE  ))
  Dr<-mutate(Dr, title = 'UCMale')

Rev<-filter(data, grepl('Rev.', Name, fixed=TRUE  ))
  Rev<-mutate(Rev, title = 'UCMale')

Ms<-filter(data, grepl('Ms.', Name, fixed=TRUE  ))
  Ms<-mutate(Ms, title = 'Mrs')

Major<-filter(data, grepl('Major.', Name, fixed=TRUE  ))
  Major<-mutate(Major, title = 'UCMale')

Col<-filter(data, grepl('Col.', Name, fixed=TRUE  ))
  Col<-mutate(Col, title = 'UCMale')

Dona<-filter(data, grepl('Dona.', Name, fixed=TRUE  ))
  Dona<-mutate(Dona, title = 'UCFemale')

Don<-filter(data, grepl('Don.', Name, fixed=TRUE  ))
  Don<-mutate(Don, title = 'UCMale')

Capt<-filter(data, grepl('Capt.', Name, fixed=TRUE  ))
  Capt<-mutate(Capt, title = 'UCMale')

Sir<-filter(data, grepl('Sir.', Name, fixed=TRUE  ))
  Sir<-mutate(Sir, title = 'UCMale')

Lady<-filter(data, grepl('Lady.', Name, fixed=TRUE  ))
  Lady<-mutate(Lady, title = 'UCFemale')

Mlle<-filter(data, grepl('Mlle.', Name, fixed=TRUE  ))
  Mlle<-mutate(Mlle, title = 'Miss')

Mme<-filter(data, grepl('Mme.', Name, fixed=TRUE  ))
  Mme<-mutate(Mme, title = 'Miss')

Ctss<-filter(data, grepl('Countess.', Name, fixed=TRUE  ))
  Ctss<-mutate(Ctss, title = 'UCFemale')

Jonk<-filter(data, grepl('Jonkheer.', Name, fixed=TRUE  ))
  Jonk<-mutate(Jonk, title = 'UCMale')

Dr<-Dr[-8, ] # remove the female Dr from 'Dr' df

FDr<-filter(data, grepl('Leader', Name, fixed=TRUE  ))
  FDr<-mutate(FDr, title = 'UCFemale')

# Create seperate title class, by sex, for people with titles indicative of the upper class
UCMale<- rbind(Dr, Rev, Sir, Major, Col, Capt, Don, Jonk)
UCFemale<- rbind(Lady, Dona, Ctss, FDr)

# combine "Ms" with "Mrs" and "Mme"/"Mlle" with Miss
Mrs<- rbind(Mrs, Ms)
Miss<- rbind(Miss, Mme, Mlle)  
  
# combine all title into one variable "title"
tbl_df(alltitles<-rbind(Mr, Mrs, Miss, Master, UCMale, UCFemale))
  glimpse (alltitles) 
  tail(alltitles)

# create dummy variable for data df
data<-mutate(data, title = "none")
glimpse(data)

data<-arrange(data, PassengerId)
head(data)

alltitles<- arrange(alltitles, PassengerId)
head(alltitles)

# add new feature "title" to data df
data$title<-alltitles$title
  summary(data)

data$title <- factor(data$title)#factorize 'title'

```

#### Survival as a function of title
```{r results ='hide'}
trainset<-data%>% arrange(dataset)%>%slice(419:1309)
head (trainset)
glimpse(trainset)


```

```{r}
hist_title <- ggplot(trainset, aes(x=title, fill=Survived))
  hist_title + geom_bar() # defaults to stacking
  hist_title + geom_bar(position= "fill") #proportions  
```

#### Verify Age range for each title group
    
```{r}
data%>%
  group_by(title)%>%
  filter(!is.na(Age))%>%
    summarise(min(Age))
data%>%
  group_by(title)%>%
  filter(!is.na(Age))%>%
  summarise(max(Age))
```

#### How many people with titles of "Mr" and "Mrs" are <=16
```{r}
under16<-filter(data, Age<=16)
under16%>%group_by(title)%>% summarise(n())
data%>%group_by(title)%>% summarise(n())
```


#### Update Child feature based on above data;assume Miss is not a Child
```{r results ='hide'}
is.na(data$Child[data$title=="Master"]<-TRUE)
is.na(data$Child[data$title=="Mr" ]<-FALSE)
is.na(data$Child[data$title=="Mrs" ]<-FALSE)
is.na(data$Child[data$title=="UCMale" ]<-FALSE)
is.na(data$Child[data$title=="UCFemale" ]<-FALSE)
is.na(data$Child[data$title=="Miss" ]<-FALSE)
```



#### *Hypothesis 3:* Data visualization suggests traveling alone decreased your odds of survival but also suggests families >=4 had decreased survival odds ##
#### Create 2 new categorical features *notalone* and *familysize*

```{r results='hide'}
data<- data %>% 
  mutate (familysize = SibSp  + Parch +1 ) %>%
  mutate(notalone = familysize >1) 
  
data$notalone<- factor(data$notalone)
  glimpse (data)
```

#### Visualize survival as a function of notalone and familysize
```{r results='hide'}
trainset<-data%>% arrange(dataset)%>%slice(419:1309)
head (trainset)
glimpse(trainset)
```


```{r}
hist_notalone <- ggplot(trainset, aes(x=notalone, fill=Survived))
  hist_notalone + geom_bar() # defaults to stacking
  hist_notalone + geom_bar(position= "fill") #proportions
```  


```{r}
hist_familysize <- ggplot(trainset, aes(x=familysize, fill=Survived))
  hist_familysize + geom_bar() # defaults to stacking
  hist_familysize + geom_bar(position= "fill") #proportions  
```


#### *Hypothesis 4:* data visualization suggests that small families had increased odds of survival ##
#### Create new categorical feature *smallfamily* from *familysize* >1 but <4 (ie between 2-4 people total)
```{r}
data$smallfamily[data$familysize >1 & data$familysize<=4] <-1
data$smallfamily[data$familysize == 1 | data$familysize>4 ] <-0
data$smallfamily <- factor(data$smallfamily)
```

#### Create feature for just 3rd Class to test as a surrogate for *Pclass*
```{r}
data$thirdClass[data$Pclass ==3 ] <-1
data$thirdClass[data$Pclass ==1 | data$Pclass==2 ] <-0
data$thirdClass <- factor(data$thirdClass)  
```

#### Visualize survival as a function of having a smallfamily or 3rd class cabin


```{r results='hide'}
trainset<-data%>% arrange(dataset)%>%slice(419:1309)
head (trainset)
glimpse(trainset)
```



```{r}
hist_smallfamily <- ggplot(trainset, aes(x=smallfamily, fill=Survived))
  hist_smallfamily + geom_bar() # defaults to stacking
  hist_smallfamily + geom_bar(position= "fill") #proportions
```



#### Visualize *thirdClass*
```{r}
hist_thirdclass <- ggplot(trainset, aes(x=thirdClass, fill=Survived))
  hist_thirdclass + geom_bar() # defaults to stacking
  hist_thirdclass+ geom_bar(position= "fill") #proportions
```
  
#### Impute value for *Age* based on logit model
```{r}
ageimp <- lm(Age~ Pclass+smallfamily+SibSp+title,  data= data)
summary(ageimp)

# assign imputed Age values for NAs in combined.df
for(i in 1:nrow(data)) {
  if(is.na(data[i, "Age"])) {
    data[i, "Age"] <- predict(ageimp, newdata = data[i, ])  
  }
}
```

#### Impute missing fare value for passenger 1044 based on median cost of thirdclass single ticket
```{r}
data<-arrange(data, desc(thirdClass))
  data<-arrange(data, SibSp)
  data<-arrange(data, Parch)

threemeanfare<-data[1:472, "Fare"]
summary(threemeanfare)

```
```{r results='hide'}
arrange(data, PassengerId)
data[59,"Fare"]<-7.854
summary(data$Fare)
```


#### Split data df into train and test datasets 
```{r}
data<-arrange(data, dataset)
test<- data[1:418, ]
class(test)
train<-data[419:1309, ]
train$Survived <- droplevels(train$Survived) 
test$Survived <- droplevels(test$Survived) 
str(test)
str(train)
```


### MACHINE LEARNING PREDICTIVE MODELING

### Logistic Regression
#### Load glm2
```{r }
library(glm2)
```

#### First perform univariate logistic regression for each important feature ## 

##### Age
```{r}
agemodel <- glm(Survived ~ Age, family="binomial", data= train)
summary(agemodel)
exp(cbind(OR = coef(agemodel), confint(agemodel))) # odds ratios and 95% CI
```

##### Sex
```{r}
sexmodel <- glm(Survived ~ Sex, family="binomial", data= train)
summary(sexmodel)
exp(cbind(OR = coef(sexmodel), confint(sexmodel))) # odds ratios and 95% CI
```

##### Cabin class
```{r}
Pclassmodel <- glm(Survived ~ Pclass, family="binomial", data= train)
summary(Pclassmodel)
exp(cbind(OR = coef(Pclassmodel), confint(Pclassmodel))) # odds ratios and 95% CI
```

##### 3rdclass
```{r}
thirdclassmodel <- glm(Survived ~ thirdClass, family="binomial", data= train)
summary(thirdclassmodel)
exp(cbind(OR = coef(thirdclassmodel), confint(thirdclassmodel))) # odds ratios and 95% CI
```

##### Sibs/spouse
```{r}
sibsmodel <- glm(Survived ~ SibSp, family="binomial", data= train)
summary(sibsmodel)
exp(cbind(OR = coef(sibsmodel), confint(sibsmodel))) # odds ratios and 95% CI
```

##### Parents/children
```{r}
Parchmodel <- glm(Survived ~ Parch, family="binomial", data= train)
summary(Parchmodel)
exp(cbind(OR = coef(Parchmodel), confint(Parchmodel))) # odds ratios and 95% CI
```

##### Fare
```{r}
faremodel <- glm(Survived ~ Fare, family="binomial", data= train)
summary(faremodel)
exp(cbind(OR = coef(faremodel), confint(faremodel))) # odds ratios and 95% CI
```

##### Embarked
```{r warning = FALSE}
embmodel <- glm(Survived ~ Embarked, family="binomial", data= train)
summary(embmodel)
exp(cbind(OR = coef(embmodel), confint(embmodel))) # odds ratios and 95% CI
```

##### Family size
```{r}
fsmodel <- glm(Survived ~ familysize, family="binomial", data= train)
summary(fsmodel)
exp(cbind(OR = coef(fsmodel), confint(fsmodel))) # odds ratios and 95% CI
```

##### Small family
```{r}
smfammodel <- glm(Survived ~ smallfamily, family="binomial", data= train)
summary(smfammodel)
exp(cbind(OR = coef(smfammodel), confint(smfammodel))) # odds ratios and 95% CI
```

##### Not alone
```{r}
namodel <- glm(Survived ~ notalone, family="binomial", data= train)
summary(namodel)
exp(cbind(OR = coef(namodel), confint(namodel))) # odds ratios and 95% CI
```

##### Child
```{r}
kidmodel <- glm(Survived ~ Child, family="binomial", data= train)
summary(kidmodel)
exp(cbind(OR = coef(kidmodel), confint(kidmodel))) # odds ratios and 95% CI
```

##### Title
```{r warning = FALSE}
titlemodel <- glm(Survived ~ title, family="binomial", data= train)
summary(titlemodel)
exp(cbind(OR = coef(titlemodel), confint(titlemodel))) # odds ratios and 95% CI
```


### Multivariable logistic regression models
```{r}
model1 <- (step(glm(Survived ~ Sex+smallfamily+notalone+Parch+Child+Age+Fare+thirdClass+SibSp, family="binomial", data= data3 ),direction= "backward"))
summary(model1)
exp(cbind(OR = coef(model1), confint(model1))) # odds ratios and 95% CI
```

```{r}
model2 <- (step(glm(Survived ~ Sex+Pclass+smallfamily+notalone+Child+Age+Fare, family="binomial", data= train ),direction= "backward"))
summary(model2)
exp(cbind(OR = coef(model1), confint(model1))) # odds ratios and 95% CI
```

```{r}
model3 <- glm(Survived ~ Sex+Pclass+smallfamily+notalone+Child, family="binomial", data= train)
summary(model3)
exp(cbind(OR = coef(model3), confint(model3))) # odds ratios and 95% CI
```
```

```{r}
model4 <- glm(Survived ~ Sex+Pclass+smallfamily+notalone+Child+Age+Fare, family="binomial", data= train)
summary(model4)
exp(cbind(OR = coef(model4), confint(model4))) # odds ratios and 95% CI
```

### Classification statistics and AUROC analysis

#### Load caret and pROC packages 
```{r }
library(caret)
library(pROC)
```

#### Model performance
```{r}
train$SurvivedYhat <- predict(model4, type = "response")  # generate yhat values on train df
train$SurvivedYhat <- ifelse(train$SurvivedYhat > 0.5, 1.0, 0.0)  # set binary prediction threshold
confusionMatrix(train$Survived,train$SurvivedYhat)  # run confusionMatrix to assess accuracy
auc(roc(train$Survived,train$SurvivedYhat))  # calculate AUROC curve 

```

#### Generate predicted values in test data for best model "model4" 
```{r}
test$Survived <- predict(model4, newdata = test, type = "response")  
test$Survived <- ifelse(test$Survived > 0.5, 1.0, 0.0)  # set binary prediction threshold

testSubmission <- data.frame(cbind(test$PassengerId, test$Survived))
colnames(testSubmission) <- c("PassengerId", "Survived")
```

#### write csv file for submission
```{r}
# save csv file for submission
write.csv(testSubmission, "Submissionlogmodel4.csv", row.names = FALSE)
```

#### RESULTS:    
**model3  (Sex+Pclass+smallfamily+notalone+Child); accuracy = 0.78947 placing 1547/3911 entries**    
**model4 (Sex+Pclass+smallfamily+notalone+Child+Age+Fare); accuracy = 0.7790, worse than model3**

### Recursive Partitioning Models
#### Load necessary packages
```{r }
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
```

#### Build the decision tree

```{r}
rpart4 <- rpart(Survived ~ Sex+Pclass+smallfamily+notalone+Child+Age+Fare+title, data = train, method ="class")
```

#### Visualize the decision tree using rpart.plot

```{r}
fancyRpartPlot(rpart4)  
```

#### Make prediction using the test set

```{r}
my_prediction <- predict(rpart4, test, type = "class")
```

#### Create a data frame with two columns for submission to Kaggle: PassengerId & Survived. 

```{r}
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
```

#### Check that my_solution has 418 entries

```{r}
nrow(my_solution)
```

#### Write csv file for submission
write.csv(my_solution, file = "rpart4.csv", row.names = FALSE)

####RESULTS:  

**rpart2: (Sex+Pclass+smallfamily+notalone+Child) accuracy = 0.79426 placing 1425/3938** 

**rpart3: (Sex+Pclass+smallfamily+notalone+Child+Age+Fare) accuracy = 0.79904 placing 1118/3938** 

**rpart4: (Sex+Pclass+smallfamily+notalone+Child+Age+Fare+title) adding title to model omits:Sex+smallfamily+notalone+Child, accuracy = 0.77990 no improvement**


### Random Forest Models
#### Load randomForest package
```{r }
library(randomForest)
```

#### Set seed for reproducibility

```{r}
set.seed(123)
```


#### Apply the Random Forest Algorithm and check variable importance
```{r}
rf4 <- randomForest(Survived ~ (Sex+Pclass+smallfamily+notalone+Child), data = train, ntree = 1000, importance = TRUE)

round(importance(rf4), 1)
```


#### Make a prediction using the test set

```{r}
my_prediction<- predict(rf4, newdata=test)
```

#### Create a data frame with two columns for submission to Kaggle: PassengerId & Survived. 
```{r}
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
```


#### Check that my_solution has 418 entries

```{r}
nrow(my_solution)
```

#### Write your solution to a csv file with the name my_solution.csv

```{r}
write.csv(my_solution, file = "rf1.csv", row.names = FALSE)
```

#### RESULTS:  

__rf2: (Sex+Pclass+smallfamily+notalone+Child+Age+Fare) accuracy = 0.80383 placing 911/3942  (Best overall model)__

**rf3: (Sex+Pclass+smallfamily+notalone+Child) accuracy = 0.79426 no improvement**

**rf4: (Sex+Pclass+smallfamily+Age+Fare) accuracy = 0.79904 no improvement**


### CONCLUSIONS
Participating in the Kaggle Titanic Competion was very rewarding and quite easy, in part because of all the resources that Kaggle provides, including datasets, scripts, notebooks, the discussion forum and for this competition several tutorials for both python and R. R is excellent open source software for data manipulation, visualization and machine learning analytics and R Studio is a very efficient and powerful platform for running R. In particular the R Markdown reporting tool, used for this report, is an excellent and efficient means of creating reports with embeded code and graphics. The relatively new *dplyr* R package for data manipulation was an improvement for this task over standard R scripting, however did not work as expected in several instances. The results of the predicitve modeling were very similar with all three methods, although the two methods which create decision trees to determine classification, recursive partitioning and random forests, were 2-3% more accurate than logistic regression. The best model was created using random forests and incorporated the features *Sex+Pclass+smallfamily+notalone+Child+Age+Fare*. This model scored 0.80383 placing 911/3942. The majority of the time involved in this project was spent preparing the data for analysis. In summary, R is a great tool for machine learning predicitve analytics. The results presented here suggest that it is most important to carefully prepare the data, including engineering additional features, and the three methods used in this study performed similarly although the random forest approach was best.        



### Titanic project Resources  
#### Kaggle Titanic homepage and  tutorials  
[www.kaggle.com/c/titanic](www.kaggle.com/c/titanic)  
[www.datacamp.com/courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic](www.datacamp.com/courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic)  
[www.youtube.com/watch?v=32o0DnuRjfg](www.youtube.com/watch?v=32o0DnuRjfg)  
[www.youtube.com/watch?v=u6sahb7Hmog](www.youtube.com/watch?v=u6sahb7Hmog)  
[rstudio-pubs-static.s3.amazonaws.com/98715_fcd035c75a9b431a84efca8b091a185f.html](rstudio-pubs-static.s3.amazonaws.com/98715_fcd035c75a9b431a84efca8b091a185f.html)    

#### dplyr resources  
[www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf](www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)  
[www.youtube.com/watch?v=jWjqLW-u3hc&feature=youtu.be](www.youtube.com/watch?v=jWjqLW-u3hc&feature=youtu.be)  
[www.youtube.com/watch?v=2mh1PqfsXVI](www.youtube.com/watch?v=2mh1PqfsXVI)  
[groups.google.com/forum/#!topic/manipulatr/Z46zwYXNh0g](groups.google.com/forum/#!topic/manipulatr/Z46zwYXNh0g)  
[stackoverflow.com/questions/22850026/filtering-row-which-contains-a-certain-string-using-dplyr](stackoverflow.com/questions/22850026/filtering-row-which-contains-a-certain-string-using-dplyr)  
[stackoverflow.com/questions/13520515/command-to-remove-row-from-a-data-frame](stackoverflow.com/questions/13520515/command-to-remove-row-from-a-data-frame)    

#### Logistic regression resources  
[cran.r-project.org/web/packages/glm2/glm2.pdf](cran.r-project.org/web/packages/glm2/glm2.pdf)  
[www.kaggle.com/eyebervil/titanic/titanic-simple-logit-with-interaction](www.kaggle.com/eyebervil/titanic/titanic-simple-logit-with-interaction)  
[cran.r-project.org/web/packages/caret/vignettes/caret.pdf](cran.r-project.org/web/packages/caret/vignettes/caret.pdf)  
[cran.r-project.org/web/packages/caret/caret.pdf](cran.r-project.org/web/packages/caret/caret.pdf)  
[cran.r-project.org/web/packages/pROC/pROC.pdf](cran.r-project.org/web/packages/pROC/pROC.pdf)  
[stats.stackexchange.com/questions/87234/aic-values-and-their-use-in-stepwise-model-selection-for-a-simple-linear-regress](stats.stackexchange.com/questions/87234/aic-values-and-their-use-in-stepwise-model-selection-for-a-simple-linear-regress)    

#### Recursive partitioning method resources  
[cran.r-project.org/web/packages/rpart/rpart.pdf](cran.r-project.org/web/packages/rpart/rpart.pdf)  
[cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf](cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)  
[campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-2-from-icebergs-to-trees?ex=1](campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-2-from-icebergs-to-trees?ex=1)  

#### Random Forest method resources  
[cran.r-project.org/web/packages/randomForest/randomForest.pdf](cran.r-project.org/web/packages/randomForest/randomForest.pdf)  
[campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-3-improving-your-predictions-through-random-forests?ex=1](campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-3-improving-your-predictions-through-random-forests?ex=1)  

Error message: *Error in randomForest.default(y = train$Survived, x = train[, c("Sex",  : 
  Can't have empty classes in y.*    
Solution: [stackoverflow.com/questions/13495041/random-forests-in-r-empty-classes-in-y-and-argument-legth-0](stackoverflow.com/questions/13495041/random-forests-in-r-empty-classes-in-y-and-argument-legth-0)  
Error message: Error in predict.randomForest(rf2, newdata = test, type = "response") : 
  No forest component in the object    
[stat.ethz.ch/pipermail/r-help/2008-June/164878.html](stat.ethz.ch/pipermail/r-help/2008-June/164878.html)  

Error Message: Error in predict.randomForest(rf2, newdata = test, type = "response") : 
  Can't predict unsupervised forest.    
Solution: [stackoverflow.com/questions/17217951/how-can-i-drop-unused-levels-from-a-data-frame](stackoverflow.com/questions/17217951/how-can-i-drop-unused-levels-from-a-data-frame)  

#### R Markdown resources  
[rmarkdown.rstudio.com/](rmarkdown.rstudio.com/)  
[www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf](www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)  
[www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf](www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  

Error message: Error: attempt to use zero-length variable name    
Solution: [stackoverflow.com/questions/31296908/knitr-running-script-without-warnings](stackoverflow.com/questions/31296908/knitr-running-script-without-warnings)  


